{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320f4d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- First 5 Predictions (Probabilities) ---\n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333194 0.33333415 0.33333391]\n",
      " [0.33333113 0.3333347  0.33333418]\n",
      " [0.33332924 0.33333608 0.33333468]\n",
      " [0.33332765 0.33333684 0.3333355 ]]\n",
      "\n",
      "--- Network Loss ---\n",
      "Loss: 1.098616900184321\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper function to create a spiral dataset\n",
    "def create_spiral_data(samples, classes):\n",
    "    \"\"\"\n",
    "    Creates a non-linear spiral dataset for classification.\n",
    "    \"\"\"\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "# ==============================================================================\n",
    "# Component 1: The Dense Layer\n",
    "# ==============================================================================\n",
    "class DenseLayer:\n",
    "    \"\"\"\n",
    "    A class representing a single, fully-connected (dense) layer in a neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights with small random values. Shape: (n_inputs, n_neurons).\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Initialize biases as zeros. Shape: (1, n_neurons).\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Store inputs for backpropagation\n",
    "        self.inputs = inputs\n",
    "        # Calculate the layer's output: (inputs • weights) + biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "# ==============================================================================\n",
    "# Component 2: The ReLU Activation Function\n",
    "# ==============================================================================\n",
    "class Activation_ReLU:\n",
    "    \"\"\"\n",
    "    A class representing the ReLU (Rectified Linear Unit) activation function.\n",
    "    \"\"\"\n",
    "    def forward(self, inputs):\n",
    "        # Store inputs for backpropagation\n",
    "        self.inputs = inputs\n",
    "        # Apply the ReLU function: output = max(0, input)\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        return self.output\n",
    "\n",
    "# ==============================================================================\n",
    "# Component 3: The Softmax Activation Function (for the output layer)\n",
    "# ==============================================================================\n",
    "class Activation_Softmax:\n",
    "    \"\"\"\n",
    "    A class representing the Softmax activation function for the output layer.\n",
    "    \"\"\"\n",
    "    def forward(self, inputs):\n",
    "        # Store inputs for backpropagation\n",
    "        self.inputs = inputs\n",
    "        # Calculate exponentiated values (with numerical stability trick)\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize the values to get probabilities\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        return self.output\n",
    "\n",
    "# ==============================================================================\n",
    "# Component 4: The Loss Function\n",
    "# ==============================================================================\n",
    "class Loss_CategoricalCrossentropy:\n",
    "    \"\"\"\n",
    "    A class to calculate the Categorical Cross-Entropy loss. ⚖️\n",
    "    \"\"\"\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        # Get the number of samples in the batch\n",
    "        n_samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Get the predicted probabilities for the correct classes\n",
    "        correct_confidences = y_pred_clipped[range(n_samples), y_true]\n",
    "        # Calculate the negative log likelihoods\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        # Calculate the average loss for the batch\n",
    "        data_loss = np.mean(negative_log_likelihoods)\n",
    "        return data_loss\n",
    "\n",
    "# ==============================================================================\n",
    "# Setup and Execution\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Create the Dataset\n",
    "# We'll create a dataset with 100 samples for each of the 3 classes.\n",
    "X, y = create_spiral_data(samples=100, classes=3)\n",
    "\n",
    "# 2. Create the Network Components\n",
    "# A 2-layer neural network.\n",
    "# Layer 1: Takes 2 inputs (from our dataset), has 64 neurons.\n",
    "dense1 = DenseLayer(2, 64) \n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Layer 2 (Output Layer): Takes 64 inputs (from dense1), has 3 output classes.\n",
    "dense2 = DenseLayer(64, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# 3. Create the Loss Function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# 4. Perform the Forward Pass\n",
    "# Pass data through the first dense layer\n",
    "dense1.forward(X)\n",
    "# Pass the result through the first activation function\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Pass the result of the first activation through the second dense layer\n",
    "dense2.forward(activation1.output)\n",
    "# Pass the result through the second activation function (Softmax)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# The output of activation2 now contains the network's predictions (probabilities)\n",
    "predictions = activation2.output\n",
    "\n",
    "# 5. Calculate the Loss\n",
    "# Compare the network's predictions with the true labels to get the loss.\n",
    "loss = loss_function.calculate(predictions, y)\n",
    "\n",
    "# 6. Print the Results\n",
    "print(\"--- First 5 Predictions (Probabilities) ---\")\n",
    "print(predictions[:5])\n",
    "print(\"\\n--- Network Loss ---\")\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# The initial loss should be approximately -log(1/n_classes).\n",
    "# For 3 classes, -log(1/3) is about 1.0986. A value close to this indicates\n",
    "# the network is starting with random, uninformed predictions, which is expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
