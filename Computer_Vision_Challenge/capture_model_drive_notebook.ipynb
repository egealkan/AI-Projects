{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous Driving Car (Trackmania)\n",
    "- This project aims to develop an autonomous driving model that can navigate a racing game environment by predicting directional commands based on real-time screen captures. The approach combines data collection, supervised learning, and computer vision techniques to train a model that classifies actions (up, down, left, and right) and performs them based on the gameâ€™s visual inputs.\n",
    "\n",
    "**Key Project Components:**\n",
    "\n",
    "- Data Collection: A script captures in-game frames with associated keypresses, creating a structured dataset by categorizing frames into folders for each action. This dataset forms the foundation for training the model.\n",
    "- Model Development: Using the FastAI library, we explored different versions of ResNet (e.g., ResNet18, ResNet34, ResNet50) with pre-trained and non-pre-trained weights, dropout regularization, and advanced techniques like differential learning rates, mixed precision, and one-cycle learning.\n",
    "- Real-Time Driving: The final script utilizes the trained model to predict actions in real time, capturing screen inputs and performing actions in response.\n",
    "- Challenges: Throughout development, we faced several key challenges, particularly in improving turn predictions (left/right) despite high accuracy metrics. Extensive experimentation with various architectures and training techniques was required to optimize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture and Log \n",
    "- This captures game frames in real-time and logs the key presses (up, down, left, right) with timestamps and simulated speed data. Each key-press frame is saved to a designated folder based on the action, creating a structured dataset for training.\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "- Ensuring data accuracy, as capturing precise, labeled game actions required balancing frame rate and keypress timing.\n",
    "- Managing a large number of captured images and organizing them by keypress categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import ImageGrab\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import random  # Simulating speed data, replace with actual speed retrieval method\n",
    "from pynput import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to save data\n",
    "BASE_FRAME_PATH = \"./data/frames/\"\n",
    "LOG_PATH = \"./data/dataset.csv\"\n",
    "\n",
    "# Ensure the frame directories exist for each key\n",
    "for key in ['up', 'down', 'left', 'right']:\n",
    "    key_dir = os.path.join(BASE_FRAME_PATH, key)\n",
    "    if not os.path.exists(key_dir):\n",
    "        os.makedirs(key_dir)\n",
    "\n",
    "# Create CSV file for logging frames, key presses, and speed\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    with open(LOG_PATH, mode='w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"timestamp\", \"frame_path\", \"key\", \"speed\"])  # Added speed to the header\n",
    "\n",
    "# Global variable to store the latest key press\n",
    "current_key = None\n",
    "\n",
    "def on_press(key):\n",
    "    \"\"\"Callback to handle key press events.\"\"\"\n",
    "    global current_key\n",
    "    try:\n",
    "        current_key = key.char  # Alphanumeric keys\n",
    "    except AttributeError:\n",
    "        # Special keys\n",
    "        if key == keyboard.Key.left:\n",
    "            current_key = 'left'\n",
    "        elif key == keyboard.Key.right:\n",
    "            current_key = 'right'\n",
    "        elif key == keyboard.Key.up:\n",
    "            current_key = 'up'\n",
    "        elif key == keyboard.Key.down:\n",
    "            current_key = 'down'\n",
    "        else:\n",
    "            current_key = None  # Ignore other keys\n",
    "\n",
    "    if current_key:\n",
    "        print(f\"Key Pressed: {current_key}\")\n",
    "\n",
    "def capture_and_log():\n",
    "    \"\"\"Capture game frames, log the key presses, and capture speed.\"\"\"\n",
    "    bbox = (0, 40, 1264, 720)  # Adjust to match your game window size\n",
    "\n",
    "    while True:\n",
    "        # Capture the screen\n",
    "        screen = np.array(ImageGrab.grab(bbox=bbox))\n",
    "        frame = cv2.cvtColor(screen, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Crop to focus only on the road section (tweak these coordinates based on the actual region of the road)\n",
    "        cropped_frame = frame[300:450, 150:1150]  # Example y,x coordinates\n",
    "\n",
    "        # Simulate speed data\n",
    "        speed = random.uniform(0, 200)  # Replace with actual speed retrieval method\n",
    "\n",
    "        # Save frame with unique timestamp\n",
    "        timestamp = str(int(time.time() * 1000))  # Milliseconds timestamp\n",
    "        \n",
    "        # Check if the current key is one of the valid keys\n",
    "        if current_key in ['up', 'down', 'left', 'right']:\n",
    "            # Save the frame to the corresponding directory\n",
    "            frame_dir = os.path.join(BASE_FRAME_PATH, current_key)\n",
    "            frame_path = os.path.join(frame_dir, f\"frame_{timestamp}.jpg\")\n",
    "            cv2.imwrite(frame_path, cropped_frame)\n",
    "\n",
    "            # Log the frame, key press, and speed\n",
    "            with open(LOG_PATH, mode='a') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([timestamp, frame_path, current_key, speed])  # Log key press and speed\n",
    "\n",
    "        # Control frame rate\n",
    "        time.sleep(0.1)  # 10 frames per second\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start listening to keyboard events\n",
    "    listener = keyboard.Listener(on_press=on_press)\n",
    "    listener.start()\n",
    "\n",
    "    # Start capturing frames and logging data\n",
    "    capture_and_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "- This defines, trains, and evaluates a ResNet-based model for action classification using FastAI. We utilize data transformations for variation, apply mixed precision for efficiency, and use a one-cycle learning rate policy to fine-tune the model.\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "- Achieving correct predictions for turns (left/right), which proved difficult even with high accuracy due to model struggles in corner prediction.\n",
    "- Iterative experiments with pre-trained vs. non-pre-trained models, dropout, resampling, and hyperparameter tuning were necessary to improve performance but did not fully resolve turn accuracy issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import os\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from fastai.metrics import accuracy, Precision, Recall, F1Score\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Paths\n",
    "BASE_FRAME_PATH = \"./data/frames/\"\n",
    "LOG_PATH = \"./data/dataset.csv\"\n",
    "\n",
    "# Define the function to extract data directly from folders\n",
    "def get_image_data():\n",
    "    \"\"\"Loads image data from folders for each label.\"\"\"\n",
    "    # Create a list of all image paths and their corresponding labels based on folder names\n",
    "    image_files = get_image_files(BASE_FRAME_PATH)\n",
    "    data = pd.DataFrame({\n",
    "        'frame_path': [str(f) for f in image_files],\n",
    "        'key': [f.parent.name for f in image_files]  # Extract the folder name as the label\n",
    "    })\n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "data = get_image_data()\n",
    "\n",
    "# Define `get_x` (image path) and `get_y` (classification labels: keyboard presses)\n",
    "def get_x(row):\n",
    "    return row['frame_path']\n",
    "\n",
    "def get_y(row):\n",
    "    return row['key'].strip()\n",
    "\n",
    "# Create a DataBlock for classification task with data loaded from folders\n",
    "block = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),  \n",
    "    get_x=get_x,\n",
    "    get_y=get_y,\n",
    "    splitter=RandomSplitter(valid_pct=0.2),  \n",
    "    batch_tfms=[\n",
    "        RandomResizedCrop(224, min_scale=0.8),  # Randomly crop and resize images directly to 224x224\n",
    "        *aug_transforms(do_flip=False, flip_vert=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the DataLoader with the balanced data\n",
    "dls = block.dataloaders(data, bs=16, num_workers=0)\n",
    "\n",
    "# Define and train the model\n",
    "learn = vision_learner(dls, resnet50, pretrained=True, metrics=[accuracy, Precision(average='weighted'), Recall(average='weighted'), F1Score(average='weighted')], ps=0.2)  # `ps` adds dropout\n",
    "\n",
    "# Find the optimal learning rate\n",
    "learn.lr_find()\n",
    "\n",
    "# Unfreeze model for training the entire architecture\n",
    "learn.unfreeze()\n",
    "\n",
    "# Fine-tune the model with one-cycle learning and differential learning rates\n",
    "learn.fit_one_cycle(50, lr_max=slice(1e-4, 1e-2))\n",
    "\n",
    "# Apply mixed precision for faster training (optional)\n",
    "learn = learn.to_fp16()\n",
    "\n",
    "# Visualize results after training\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix(figsize=(8, 8), dpi=100)\n",
    "interp.plot_top_losses(9, figsize=(15, 10))\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "learn.export('./models/model_classification_fastai_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture and Drive\n",
    "- This allows real-time model inference to control the car based on screen captures, predicting and executing actions in the game. It uses the trained model to make predictions and sends keypress signals to guide the car.\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "- Synchronizing frame capture with game speed to ensure predictions are made quickly enough for responsive driving.\n",
    "- Consistent mispredictions on turns, often leading the car to drive straight instead of adjusting for corners, revealed limitations in training data and model adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import torch\n",
    "from pynput.keyboard import Controller, Key\n",
    "from PIL import ImageGrab\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine custom functions used during training\n",
    "def get_x(row):\n",
    "    return row['frame_path']\n",
    "\n",
    "def get_y(row):\n",
    "    return row['key'].strip()\n",
    "\n",
    "# Load the trained model\n",
    "model_path = './models/model_classification_fastai_v2.pkl'\n",
    "learn = load_learner(model_path)\n",
    "# print(learn.dls.vocab)\n",
    "\n",
    "# Initialize keyboard controller\n",
    "keyboard = Controller()\n",
    "\n",
    "# Function to capture frames and control the car\n",
    "def capture_and_drive():\n",
    "    bbox = (0, 40, 1280, 720)  # Adjust based on your screen setup\n",
    "\n",
    "    while True:\n",
    "        # Capture the game screen\n",
    "        screen = np.array(ImageGrab.grab(bbox=bbox))\n",
    "\n",
    "        # Convert to RGB and then to PIL image for FastAI\n",
    "        frame = cv2.cvtColor(screen, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        frame = PILImage.create(frame)\n",
    "\n",
    "        # Resize to match training size\n",
    "        frame = Resize(224)(frame)\n",
    "\n",
    "        # Convert PIL image to tensor using ToTensor\n",
    "        frame_tensor = ToTensor()(frame)  # Convert to tensor\n",
    "\n",
    "        # Normalize the tensor to [0, 1] range (optional: depending on training settings)\n",
    "        frame_tensor = frame_tensor.float() / 255.0  # Convert to float and normalize if required\n",
    "\n",
    "        # Add batch dimension and move to device (GPU/CPU)\n",
    "        frame_tensor = frame_tensor.unsqueeze(0).to(learn.dls.device)\n",
    "\n",
    "        # Predict action using the loaded model\n",
    "        preds = learn.model(frame_tensor)\n",
    "        predicted_action = preds.argmax().item()\n",
    "\n",
    "        # Convert index to action\n",
    "        action = learn.dls.vocab[predicted_action]\n",
    "        print(f\"Predicted action: {action}\")\n",
    "\n",
    "        # Perform the predicted action\n",
    "        if action == 'left':\n",
    "            keyboard.press(Key.left)\n",
    "            time.sleep(0.1)\n",
    "            keyboard.release(Key.left)\n",
    "        elif action == 'right':\n",
    "            keyboard.press(Key.right)\n",
    "            time.sleep(0.1)\n",
    "            keyboard.release(Key.right)\n",
    "        elif action == 'up':\n",
    "            keyboard.press(Key.up)\n",
    "            time.sleep(0.1)\n",
    "            keyboard.release(Key.up)\n",
    "        elif action == 'down':\n",
    "            keyboard.press(Key.down)\n",
    "            time.sleep(0.1)\n",
    "            keyboard.release(Key.down)\n",
    "\n",
    "        # Control frame rate\n",
    "        time.sleep(0.2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    capture_and_drive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_project_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
